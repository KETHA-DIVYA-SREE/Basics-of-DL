{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of output feature map after convolution\n",
    "- no. of filters or kernels = K\n",
    "- size of filters or kernels = m X m\n",
    "    - Wout = ((Win - m + 2 * Padding) / stride) + 1\n",
    "    - Hout = ((Hin - m + 2 * Padding) / stride) + 1\n",
    "    - Dout = K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Conv2d() as images are 2D\n",
    "size of image (number of samples, Channels, width, height) \n",
    "--> channels(gray: 1, color: 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 64, 64),\n",
       " tensor([[[[0.9280, 0.9214, 0.5885,  ..., 0.3654, 0.6954, 0.0538],\n",
       "           [0.5567, 0.8248, 0.3327,  ..., 0.8213, 0.6177, 0.6119],\n",
       "           [0.6789, 0.6300, 0.9344,  ..., 0.2386, 0.1947, 0.3920],\n",
       "           ...,\n",
       "           [0.8905, 0.3566, 0.6389,  ..., 0.7166, 0.3440, 0.8880],\n",
       "           [0.8487, 0.0968, 0.2288,  ..., 0.8974, 0.7104, 0.1544],\n",
       "           [0.6052, 0.4858, 0.1514,  ..., 0.2451, 0.6716, 0.8090]],\n",
       " \n",
       "          [[0.3753, 0.2328, 0.7849,  ..., 0.3571, 0.7894, 0.5515],\n",
       "           [0.8181, 0.6191, 0.6249,  ..., 0.0952, 0.6396, 0.4197],\n",
       "           [0.8064, 0.6694, 0.5372,  ..., 0.7785, 0.8265, 0.8231],\n",
       "           ...,\n",
       "           [0.6312, 0.9805, 0.8453,  ..., 0.0183, 0.0208, 0.6508],\n",
       "           [0.1023, 0.0306, 0.5430,  ..., 0.2111, 0.7842, 0.5276],\n",
       "           [0.3836, 0.3381, 0.8210,  ..., 0.3478, 0.9811, 0.9253]],\n",
       " \n",
       "          [[0.6528, 0.9186, 0.0859,  ..., 0.6745, 0.0930, 0.7698],\n",
       "           [0.8396, 0.9616, 0.6769,  ..., 0.1893, 0.4712, 0.5681],\n",
       "           [0.1424, 0.5743, 0.8171,  ..., 0.5741, 0.6692, 0.8457],\n",
       "           ...,\n",
       "           [0.0829, 0.5877, 0.3349,  ..., 0.1470, 0.5211, 0.3229],\n",
       "           [0.9719, 0.4939, 0.0838,  ..., 0.7264, 0.5685, 0.4661],\n",
       "           [0.6740, 0.7612, 0.2065,  ..., 0.4991, 0.3826, 0.5246]]]]),\n",
       " torch.Size([1, 3, 64, 64]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = (1, 3, 64, 64) # Convolution layer need 4D shape\n",
    "imgT = torch.rand(img) # Normalize\n",
    "img, imgT, imgT.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the parameters and create the instance of nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3 # RGB or color\n",
    "out_channels = 10 # number of kernel -- K\n",
    "kernel_size = 5 # m\n",
    "stride = 1\n",
    "padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convLayer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 62, 62])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_convLayer = convLayer(imgT)\n",
    "out_convLayer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose of Convolution (nn.ConvTranspose2d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 10\n",
    "out_channels = 3\n",
    "kernel_size = 5\n",
    "stride = 1\n",
    "padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_Transpose_Layer = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_conv_Transpoze_Layer = conv_Transpose_Layer(out_convLayer)\n",
    "out_conv_Transpoze_Layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.MaxPool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 2\n",
    "stride = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = nn.MaxPool2d(kernel_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 62, 62])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_convLayer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 31, 31])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_max_p = max_p(out_convLayer)\n",
    "out_max_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.flatten()\n",
    "- 4D to 2D\n",
    "- To perform classification using Linear or Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1440])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.rand(1, 10, 12, 12)\n",
    "out = torch.flatten(input, 1) # torch.flatten(input, dimention)\n",
    "out.shape # (1, 10*12*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
